#!/usr/bin/env ruby
# frozen_string_literal: true

#
#  USAGE::
#
#    script/refresh_sitemap
#
#  DESCRIPTION::
#
#  Build robots.txt, a set of sitemap.xml files, and a set of pared-down
#  index.html files for robots to use to index our site efficiently.
#
#  Creates the following files:
#
#    RAILS_ROOT/public/robots.txt
#    RAILS_ROOT/public/sitemap/index.xml
#    RAILS_ROOT/public/sitemap/static.xml
#    RAILS_ROOT/public/sitemap/<table>-<N>.xml
#    RAILS_ROOT/public/sitemap/index.html
#    RAILS_ROOT/public/sitemap/static.html
#    RAILS_ROOT/public/sitemap/<table>-<N>.html
#
#  The robots.txt file tells robots what *not* to crawl.  The sitemap.xml
#  files give search robots useful information about the files we *do* want
#  them to crawl (e.g., how often to crawl them, or when they have changed).
#  The sitemap.html files give robots access to the pages we *do* want them
#  to crawl (since we can't always guarantee that they will be able to reach
#  all those pages via pages they're allowed to crawl by robots.txt).
#
################################################################################

require(File.expand_path("../config/boot.rb", __dir__))
require(File.expand_path("../config/environment.rb", __dir__))

DOMAIN      = "https://mushroomobserver.org"
SITEMAP_URL = "#{DOMAIN}/sitemap/NAME.xml"
INDEX_URL   = "#{DOMAIN}/sitemap/NAME.html"

ROBOTS_FILE     = "#{::Rails.root}/public/robots.txt"
SITEMAP_FILE    = "#{::Rails.root}/public/sitemap/NAME.xml"
INDEX_FILE      = "#{::Rails.root}/public/sitemap/NAME.html"
CONTROLLER_FILE = "#{::Rails.root}/app/controllers/NAME_controller.rb"

OBJECT_TYPES = [
  ["observations",
   "Observation",
   "observer/show_observation/ID"],
  ["names",
   "Name",
   "name/show_name/ID"],
  ["locations",
   "Location",
   "location/show_location/ID"],
  ["species_lists",
   "SpeciesList",
   "species_list/show_species_list/ID"],
  ["projects",
   "Project",
   "project/show_project/ID"],
  ["glossary_terms",
   "GlossaryTerm",
   "glossary_terms/ID"],
  ["herbaria",
   "Herbarium",
   "herbaria/show/ID"],
  ["name_descriptions",
   "NameDescription",
   "name/show_name_description/ID"],
  ["location_descriptions",
   "LocationDescription",
   "location/show_location_description/ID"]
].freeze

STATIC_PAGES = [
  ["home page",    "observer/list_rss_logs", "hourly"],
  ["introduction", "observer/intro",         "weekly"],
  ["how to use",   "observer/how_to_use",    "weekly"],
  ["how to help",  "observer/how_to_help",   "weekly"]
].freeze

# These are aliases for pages allowed above.
OTHER_ALLOWED_PATHS = [
  # "/observer/index", (would allow too many indexes we don't want to allow)
  "/observer/lookup_observation",
  "/observer/lookup_name",
  "/observer/show_name",
  "/name/eol"
].freeze

LOCALES = Language.connection.select_values(%(
  SELECT locale FROM languages WHERE NOT beta ORDER BY `order` ASC
))

# Control behavior of crowlers that follow robots.txt rules
# For documentation, see:
# https://developers.google.com/search/docs/advanced/robots/robots_txt
# http://www.robotstxt.org

# Other ways for MO to block/control robots include:
# - Manually block ip address ranges at the firewall
# - Automatically block in Rails an anonymous user (including robots)
#   that's hogging the webserver. See script/update_ip_stats.rb
# - Automatically block in Rails a fake GoogleBot.
#    See script/update_googlebots.rb
# - Manually block in Rails an anonymous user via the Admin UI
# - Limit them via browser gem and #bot? method
ROBOTS_HEADER = <<~"TXT"

  Sitemap: #{SITEMAP_URL.sub("NAME", "index")}

  # Block non-whitelisted crawlers
  # Google AdBot crawlers must be named explicitly
  # https://developers.google.com/search/docs/advanced/robots/create-robots-txt
  User-agent: AdsBot-Google
  User-agent: AdsBot-Google-Mobile
  User-agent: AdsBot-Google-Mobile-Apps
  User-agent: Facebot # Unofficial Facebook bots (subset of facebookexternalhit)
  User-agent: * # Block everything else not whitelisted below
  Disallow: /

  # Whitelisted crawlers
  # All are assume to use wildcards in their robots.txt parser
  User-agent: bingbot
  User-agent: CCBot
  User-agent: GoogleBot
  User-agent: facebookexternalhit
  Crawl-Delay: 15
  Disallow: *page=*
  Disallow: *letter=*
  Disallow: *by=*
  Disallow: *user_theme=*
  Disallow: *set_thumbnail_size=*
  Disallow: */show*user_locale=*
  Disallow: */index*user_locale=*
  Disallow: */lookup*user_locale=*
  Disallow: /*?*q= # block urls having the q parameter
  Disallow: /name/show_name

TXT
# The rest of #write_robots_file mjust follow immeediately
# abd aookues to the group of whitelisted user agents immediatelyu above.
ROBOTS_FOOTER = <<"TXT"
TXT

SITEMAP_ROOT_HEADER = <<~"XML"
  <?xml version="1.0" encoding="UTF-8"?>
  <sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
XML
SITEMAP_ROOT_FOOTER = <<~"XML"
  </sitemapindex>
XML

SITEMAP_LEAF_HEADER = <<~"XML"
  <?xml version="1.0" encoding="UTF-8"?>
  <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
XML
SITEMAP_LEAF_FOOTER = <<~"XML"
  </urlset>
XML

INDEX_HEADER = <<~"HTML"
  <html><head></head><body>
HTML
INDEX_FOOTER = <<~"HTML"
  </body></html>
HTML

################################################################################

def update_sitemap_files
  FileUtils.mkpath(File.dirname(SITEMAP_FILE))
  last_update = formatted_last_update
  comments = get_new_comments(last_update)
  sitemaps = []
  OBJECT_TYPES.each do |object_type|
    sitemaps += map_one_object_type(object_type, comments, last_update)
  end
  sitemaps += map_static_pages
  write_index_xml(sitemaps)
  write_index_html(sitemaps)
end

def get_new_comments(last_update)
  Name.connection.select_rows(%(
    SELECT target_type, target_id, updated_at
    FROM comments WHERE updated_at >= "#{last_update}"
  ))
end

def get_new_images(last_update)
  Name.connection.select_rows(%(
    SELECT io.observation_id, i.updated_at
    FROM images i, images_observations io
    WHERE i.id = io.image_id AND i.updated_at >= "#{last_update}"
  ))
end

def map_one_object_type(params, comments, last_update)
  table, type, path = *params
  times = read_old_files(table)
  update_times_of_new_objects(times, table, last_update)
  if type == "Observation"
    update_times_of_objects_with_new_images(times, last_update)
  end
  update_times_of_objects_with_new_comments(times, comments, type)
  write_new_files(times, table, path)
end

def update_times_of_new_objects(times, table, last_update)
  Name.connection.select_rows(%(
    SELECT id, updated_at FROM #{table} WHERE updated_at > "#{last_update}"
  )).each do |id, time|
    times[id.to_i] = parse_mysql_time(time)
  end
end

def update_times_of_objects_with_new_images(times, last_update)
  get_new_images(last_update).each do |id, time|
    id = id.to_i
    time = parse_mysql_time(time)
    times[id] = time if !times[id] || times[id] < time
  end
end

def update_times_of_objects_with_new_comments(times, comments, type)
  comments.each do |target_type, target_id, target_time|
    target_id = target_id.to_i
    target_time = parse_mysql_time(target_time)
    if target_type == type && (!times[target_id] ||
                               times[target_id] < target_time)
      times[target_id] = target_time
    end
  end
end

def read_old_files(table)
  result = {}
  pat = Regexp.new("<loc>.*/(\\d+)</loc><lastmod>(.*)</lastmod>")
  wildcard = SITEMAP_FILE.sub("NAME", "#{table}-*")
  Dir.glob(wildcard).each do |file|
    File.open(file).readlines.each do |line|
      if (match = pat.match(line))
        result[match[1].to_i] = match[2]
      end
    end
  end
  result
end

def write_new_files(times, table, path_template)
  pages = []
  times.keys.sort.each do |id|
    str  = "#{table} ##{id}"
    path = path_template.sub("ID", id.to_s)
    time = "<lastmod>#{times[id]}</lastmod>"
    pages << [str, path, time]
  end
  sitemaps = []
  until pages.empty?
    name = "#{table}-#{sitemaps.length + 1}"
    next_pages = pages[0..9999]
    write_sitemap_xml(name, next_pages)
    write_sitemap_html(name, next_pages)
    pages[0..9999] = []
    sitemaps << name
  end
  sitemaps
end

def map_static_pages
  name = "static"
  pages = []
  STATIC_PAGES.each do |str, path, freq|
    LOCALES.each do |locale|
      lang = locale == "en" ? "" : "?user_locale=#{locale}"
      time = "<changefreq>#{freq}</changefreq>"
      pages << ["#{str} in #{locale}", "#{path}#{lang}", time]
    end
  end
  write_sitemap_xml(name, pages)
  write_sitemap_html(name, pages)
  [name]
end

def write_sitemap_xml(name, pages)
  file = SITEMAP_FILE.sub("NAME", name)
  File.open(file, "w") do |fh|
    fh.write(SITEMAP_LEAF_HEADER)
    pages.each do |_str, path, time|
      fh.puts("<url><loc>#{DOMAIN}/#{path}</loc>#{time}</url>")
    end
    fh.write(SITEMAP_LEAF_FOOTER)
  end
end

def write_sitemap_html(name, pages)
  file = INDEX_FILE.sub("NAME", name)
  File.open(file, "w") do |fh|
    fh.write(INDEX_HEADER)
    pages.each do |str, path, _time|
      fh.puts("<a href=\"/#{path}\">#{str}</a><br/>")
    end
    fh.write(INDEX_FOOTER)
  end
end

def write_index_xml(sitemaps)
  now = formatted_current_time
  file = SITEMAP_FILE.sub("NAME", "index")
  File.open(file, "w") do |fh|
    fh.write(SITEMAP_ROOT_HEADER)
    sitemaps.each do |sitemap|
      url  = "<loc>#{SITEMAP_URL.sub("NAME", sitemap)}</loc>"
      time = "<lastmod>#{now}</lastmod>"
      fh.puts("<sitemap>#{url}#{time}</sitemap>")
    end
    fh.write(SITEMAP_ROOT_FOOTER)
  end
end

def write_index_html(sitemaps)
  file = INDEX_FILE.sub("NAME", "index")
  File.open(file, "w") do |fh|
    fh.write(INDEX_HEADER)
    sitemaps.each do |sitemap|
      fh.puts("<a href=\"#{sitemap}.html\">#{sitemap}</a><br/>")
    end
    fh.write(INDEX_FOOTER)
  end
end

################################################################################

# Times come out of the mysql adapter as Time objects right now.
# Sitemaps require "yyyy:mm:ddThh:mm:ss+00:00" format.
# These functions ensure that all times are Strings in that format.
def parse_mysql_time(time)
  format_time(time)
end

def format_time(time)
  time.utc.strftime("%Y-%m-%dT%H:%M:%S+00:00")
end

def formatted_last_update
  file = SITEMAP_FILE.sub("NAME", "index")
  begin
    format_time(File.mtime(file))
  rescue StandardError
    format_time(100.years.ago)
  end
end

def formatted_current_time
  format_time(Time.zone.now)
end

################################################################################

def update_robots_file
  allowed_actions = deduce_allowed_actions
  disallowed_paths = []
  list_of_controllers.each do |controller|
    actions  = controller_actions(controller)
    allow    = allowed_actions[controller] || []
    disallow = actions - allow
    disallowed_paths += disallow.map { |action| "/#{controller}/#{action}" }
  end
  allowed_paths = flatten_allowed_actions(allowed_actions) + OTHER_ALLOWED_PATHS
  remove_substrings_from_disallowed_paths(disallowed_paths, allowed_paths)
  write_robots_file(disallowed_paths, allowed_paths)
end

def deduce_allowed_actions
  actions = {}
  OBJECT_TYPES.each do |_table, _type, path|
    add_action(actions, path)
  end
  STATIC_PAGES.each do |_str, path, _freq|
    add_action(actions, path)
  end
  actions
end

def add_action(actions, path)
  controller, action = parse_path(path)
  actions[controller] ||= []
  actions[controller] << action
end

def parse_path(path)
  controller, action = path.split("/")
  action&.sub!(/\?.*/, "")
  [controller, action]
end

def list_of_controllers
  # We could use ApplicationController.descendants, but controllers are loaded
  # lazily, so there won't actually be any loaded yet.  This is safer.
  result = []
  wildcard = CONTROLLER_FILE.sub("NAME", "*")
  Dir.glob(wildcard).each do |file|
    match = file.match(/(\w+)_controller.rb/)
    result << match[1] if match
  end
  result
end

def controller_actions(controller)
  klass = "#{controller}_controller".camelize.constantize
  instance = klass.new
  klass.action_methods.reject do |name|
    name.match(/\W/) || instance.method(name).arity != 0
  end
end

def flatten_allowed_actions(allowed_actions)
  results = []
  allowed_actions.keys.sort.each do |controller|
    allowed_actions[controller].uniq.each do |action|
      results << "/#{controller}/#{action}"
    end
  end
  results
end

def remove_substrings_from_disallowed_paths(disallowed_paths, allowed_paths)
  disallowed_paths.reject! do |disallowed_path|
    reject = false
    allowed_paths.each do |allowed_path|
      # Disable cop because here ".zero?" is not the same as " == 0";
      # If called on nil: .zero? throws an error, while ==0 returns "false"
      # Here nil is a perfectly acceptable return value
      # (means str2 is not found in str1)
      # rubocop:disable Style/NumericPredicate
      next unless allowed_path.index(disallowed_path) == 0

      # rubocop:enable Style/NumericPredicate
      reject = true
      break
    end
    reject
  end
end

def write_robots_file(disallowed_paths, allowed_paths)
  File.open(ROBOTS_FILE, "w") do |fh|
    fh.write(ROBOTS_HEADER)
    disallowed_paths.sort.each do |path|
      fh.puts("Disallow: #{path}")
    end
    allowed_paths.each do |path|
      fh.puts("# Allow: #{path}")
    end
    fh.write(ROBOTS_FOOTER)
  end
end

################################################################################

update_sitemap_files
update_robots_file
exit(0)
